{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Using some observed data as input, we can train a deep neural network to predict things such as the classes of that data, or the parameter values of that data. We will do this using various modules from the `keras` python deep learning API package. Similar to what we have seen in both the unsupervised and the supervised case, you can think of a deep neural network as essentially a function approximater, such as\n",
    "\n",
    "$$ y = f (x) $$\n",
    "\n",
    "where $y$ is the output of the deep neural network and $x$ is the data. In the deep learning case, both the data $x$ and the labels $y$ may be either known or unknown depending on the type of network you are using (supervised/unsupervised). The complexity of the function that we use to map our input $x$ to our output $y$ is entirely up to you, the user. We will discuss below the various different types of deep learning algorithms that are available to you and all the many knobs (hyperparameters) you can tune to solve your own problems.\n",
    "\n",
    "It's a very exciting time to be doing deep learning, so have fun!\n",
    "\n",
    "<img src=\"images/ml_overview.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Estimating the Frequency of a Sine Wave using Fully-Connected Neural Networks\n",
    "\n",
    "In this exercise, we will be using a simple fully-connected deep neural network much like the one seen in the figure below.\n",
    "\n",
    "<img src=\"images/fully_connected_nn.png\">\n",
    "\n",
    "A single neuron in this network can be described mathematically by\n",
    "\n",
    "$y_i = \\sigma(w_1 \\times x_1 + ... + w_m \\times x_m )$,\n",
    "\n",
    "where $x_m$ is the mth input of some batch of training data, $y_i$ is the ith neuron in a layer and $w$ is the corresponding weight for mth input to $y_i$. In this exercise, our input, $x$, will be a set of 1-D sine-wave signals. Each neuron is fully-connected to all neurons in the previous layer, as well as all neurons in the next layer. This basically means that information flows freely between all neurons in the deep neural network.\n",
    "\n",
    "What we will be trying to predict in the output layer of our fully-connected neural network, given a set of sine wave signals, will be some parameters, $y_i$,  which describe a sine wave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define some hyperparameters (knobs you can turn) which will affect the accuracy and behavior of the network during training. You can leave them as is for now, but you may come back to the cell below later to change them as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some global parameters\n",
    "max_freq = 10.   # maximum allowed frequency of the sine-wave\n",
    "Nsamples = 200   # sampling frequency above nyquist of sine wave signals\n",
    "Nsignals = 10000 # total number of sine wave signals to generate\n",
    "\n",
    "# network params\n",
    "epochs = 20     # total number of training epochs\n",
    "batch_size = 64 # number of signals to send into neural network per training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "I have provided below a general function which takes as input frequency and phase that we will be using to generate sine-wave signals. However, if you're feeling particularly keen, you may also try other functions such as sine-Gaussian waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_sine(frequency, phase):\n",
    "    \"Generate a sinusoidal signal with a given frequeny and phase shift\"\n",
    "    return np.sin(frequency * np.linspace(phase, phase + 2. * np.pi, Nsamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Data\n",
    "\n",
    "Below, I have provided code that will generate both the sine-wave signals $X$ and the parameters of those signals $Y$ in a format that will be easy for the deep neural network read in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate parameter labels\n",
    "Y = np.array([np.random.uniform(0.1,max_freq,Nsignals),np.random.uniform(0,1,Nsignals)]).T\n",
    "\n",
    "# iterate through parameters and generate sine waves based off of those parameters\n",
    "X = np.array([gen_sine(y[0], y[1]) for y in Y])\n",
    "\n",
    "\n",
    "# split data into train/test sets\n",
    "\n",
    "# get a random set of indices\n",
    "train_idx = np.random.randint(X.shape[0], size=int(0.9*X.shape[0])) # 90% of signals for training\n",
    "test_idx = np.random.randint(X.shape[0], size=int(0.1*X.shape[0]))  # 10% of signals for testing\n",
    "\n",
    "# split the data give indices defined above\n",
    "x_train, x_test = X[train_idx,:], X[test_idx,:]\n",
    "y_train, y_test = Y[train_idx,:], Y[test_idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have both your input data $X$ and your parameter labels for that input data $Y$, make a few plots of the sine-waves contained in $X$. Is there enough variation from signal to signal? Is the frequency too high? Is the length of the time series long enough? \n",
    "\n",
    "You shouldn't find any problems, but as a machine learner, it's always best practice to double check the data that your using for training/testing your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're satisfied with the quality of the input that you will be using, we can set up our deep fully-connected neural network model. To do this, please refer to the following [example](https://keras.io/getting-started/sequential-model-guide/) under `Getting started with the Keras Sequential model` for pointers on how to code your own neural network model. \n",
    "\n",
    "If the instructions are a bit too vague, feel free to ask a demonstrater for help. :) \n",
    "\n",
    "Some tips:\n",
    "-  Use the `Sequential()` model.\n",
    "-  Start out with 1-2 fully-connected hidden layers (i.e. `model.add(Dense(num_neurons))`).\n",
    "-  Use two neurons in the final `Dense` layer with a `linear` activation function. Each neuron represents a different parameter of the sine wave we are trying to predict. We use a `linear` activation function because we want to allow the neural network to predict any value it wants (e.g. no bounds on output).\n",
    "-  I would start with using `relu` activation functions in the hidden layers. \n",
    "-  `model.summary()` will return a summary of the structure of your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your model using the `model.compile()` keras function. \n",
    "\n",
    "-  Use the `mean_squared_error` loss function, since we are trying to minimize the difference between the neural network predicted parameters and the true sine-wave parameters. \n",
    "-  You can choose whatever optimizer you want, but I would stick with stochastic gradient descent (SGD). Your optimizer determines how the deep neural network will explore the loss function space and will attempt to try and minimize said loss function.\n",
    "-  make sure to set the `metrics` variable in `model.compile` equal to `['accuracy']`. This will make it so that you can see how accurate your neural network is while it is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model using the keras `model.fit()` function and then evalaute the accuracy of your model using the `model.evaluate()` function. \n",
    "\n",
    "-  Go ahead and set the output of `model.fit()` equal to some variable (I would call it `history`). This `history` variable will contain a history of the training, which includes the loss (which you are trying to minimize). \n",
    "-  Set the output of the `model.evaluate` function to some variable (could call it `score`). Printing `score` will return the overall loss of the network after testing on new data it hasn't seen before, as well as the accuracy.\n",
    "-  Try to aim for a loss during testing of ~0.03 an accuracy of ~97%. See keras documentation for further details on fitting function. [link](https://keras.io/models/model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the values for the parameter predictions from the deep neural network using the keras `model.predict()` function. Execute this using the `x_test` data variable (this is your testing dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the deep neural network predicted values in a scatter plot as a function of the true values for each parameter. If the predictions are exactly the true value, we would expect a straight line. We actually end up seeing a line with some spread, the more spread the less accurate the predictions.\n",
    "\n",
    "In this case, the periodic nature of the sine wave makes estimating the phase shift $\\phi$ hard, but the frequency $f$ is easy enough for the network to predict. Feel free to play around with the neural network to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Classification of the MNIST Digits Dataset using Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks (CNNs) are typically composed of layers of 2-dimensional nuerons known as filters. Similar to a fully-connected deep neural network, each one of these filters is also fully-connected to all the filters in both the next and previous convolutional layer. The output of 2D filters is passed from one hidden layer to the next (red box in figure below) until the flattening layer where the output is flattened into a 1D array (blue box in figure below). The output is then forwarded through some fully-connected layers and finally to the output layer, where predictions are made. \n",
    "\n",
    "<img src=\"images/cnn_nn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will be using MNIST, which is a handwrriten digit database with a training set of 60,000 handwritten 2D digital images and a test set of 10,000 handwritten 2D digital images. The images are all 28x28 pixels and in a greyscale color format. It is available [here](http://yann.lecun.com/exdb/mnist/), but is also included in Keras and will automatically download when applying the instructions below.\n",
    "\n",
    "<img src=\"images/MNIST_ex.png\">\n",
    "\n",
    "First, load in the MNIST data using the `mnist.load_data` keras module [mnist load data ex](https://keras.io/datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you should end up with two sets of numpy arrays which are the training/testing images (`x_train`,`x_test`) and two sets of arrays which are the labels (`y_train`,`y_test`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting 9 of your training images with the `matplotlib` [subplot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html) and [imshow](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html) modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some general hyperparameters which we will be using during training below. You can change the values of these hyperparameters later on (I wouldn't suggest changing the number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128    # number of images passed each iteration\n",
    "num_classes = 10    # digits 0 to 9\n",
    "epochs = 20         # number of full passes of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CNN, the data needs to be parsed as a 2D array, rather than a 1D array. Reshape your training/testing images to be of the shape (number_samples,number_pixels,number_pixels,1). Additionally, it is generally good practice to normalize your training/testing set such that the image array values range from $0$ to $1$. Normalizing your set will reduce the space over which the convolutional neural network has to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# reshape data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255                           # normalize training set. Use 255 because that is max of train set\n",
    "x_test /= 255                            # normalize testing set Use 255 because that is max of test set\n",
    "print('x_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the keras module `keras.utils.to_categorical` to convert your training/testing labels from integer to binary class matrices, also known as one-hot encoding (e.g. from class ($0$) to ($1$,$0$,$0$),from class ($1$) to ($0$,$1$,$0$), and from class ($2$) to ($0$,$0$,$1$)). Remember, the number of classes you will be training over is $10$.\n",
    "\n",
    "Google `keras.utils.to_categorial` to see how this is done, or feel free to ask a demonstrater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now finally gotten to stage where we can define the structure of our deep convolutional neural network module. Refer to the **Building the Module** section of the following link for an example on how to build a convolutional neural network model. [link](https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5)\n",
    "\n",
    "**Tips**\n",
    "-  For the `input_shape` variable in your first hidden layer, use the `input_shape` variable defined in the reshaping cell you executed above.\n",
    "-  Use 2-3 hidden `Conv2D` layers. The more you use, the more complex your network will be.\n",
    "-  Try using a `MaxPooling2D` layer. Helps get the most important features out of your convolutional filters. See [link](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) for more info.\n",
    "-  Try using a `Dropout` layer. This will drop a certain percentage of connections between convolutional filters during training. Dropout helps the neural network generalize to new data it hasn't seen before.\n",
    "-  Use 1-2 `Dense` hidden layers. The more you use, the more complex your network will be.\n",
    "-  Make sure your final layer is a `Dense` layer with the number of neurons equal to `num_classes` and has a `softmax` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different hyperparameters that you can play with. I would try adjusting the following after having gone through one training session of your network (we will do this in the cells below):\n",
    "-  Dropout\n",
    "-  Number of hidden layers\n",
    "-  Max pooling\n",
    "-  Number of filters/neurons per layer\n",
    "-  Activation functions in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the CNN model using the keras `model.compile` function. See the **compiling the model** section in the following link for an example on how to apply this: [link](https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5)\n",
    "\n",
    "**Tips**\n",
    "-  Use a categorical_crossentropy loss\n",
    "-  Try different optimizers. Each have their own hyperparameters you can tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model on the training data/labels using the keras `model.fit` function. Once your model has been trained, you can test the accuracy of your model using the keras `model.evaluate` function. The fitting function will return the loss of the neural network after training and the evaluate function will return an accuracy percentage score of the neural network. See the `training the model` section in the following link for an example on how to apply the `model.fit` function: [link](https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5)\n",
    "\n",
    "Ideally, you should be aiming for 95%+ classification accuracy. Keep tunning the network hyperparameters until you get a good score. This is the fun part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Text Generation using Long Short Term Memory (LSTM) Networks\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are an improvement on recurrent neural networks (RNN), which basically extend their memory. Because of this, it's is well suited to learn from important experiences that have very long time lags in between. The units of an LSTM are used as building units for the layers of a RNN, which is then often called an LSTM network.\n",
    "\n",
    "LSTMs also contain something known as memory (much like RAM in a computer). This memory is usually known as a gated cell, where gated means whether or not the cell can store deleted information. Weights are assigned in order to determine the importance of what bits of information to keep or discard.\n",
    "\n",
    "In LSTMS, we have three gates: input, forget, and the output gate. The gates determine whether or not we allow new input in (input gate), delete unimportant information (forget gate), or allow the input information to affect the output of the network (output gate). The gates are typically in the form of a sigmoid function which has ranges from $0$ to $1$ (see figure below). If we put enough of these units together, we can create a deep neural network which will make predictions on sequentially dependent data!\n",
    "\n",
    "<img src=\"images/LSTM_unit.png\">\n",
    "\n",
    "In the exercise below, you will take a dataset containing some of Nietzche's writings and make an LSTM which will produce Neitzche-like text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset to be used for this first exercise is a freely avaiable text dataset of [Nietzsche's writings](https://s3.amazonaws.com/text-datasets/nietzsche.txt) you can download. We will use the keras data utility `get_file` to download the text. This dataset will need spliting up into smaller sequences that can then be used to train the LSTM.  Load in this data and inspect it.\n",
    "\n",
    "* What is the shape of the data?\n",
    "* What does this tell you about the number of features that describes the data?\n",
    "* How many different types of data are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded in the dataset, we should figure out the total number of characters in the dataset. You can do this by turning the `text` variable into a python set and that set into list using both python built-in function `list` and `set`. You can order the text using the built-in python function `sorted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many different characters is that?\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the dataset up into a list of sub-sequences, which is what the LSTM requires as input. Each one of these subsequences will have a max length of 40 characters. In addition, make a seperate list containing the next character that follows each subsequence. We make sure to leave at least 40 characters at the end of the dataset to be set aside as testing data to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut the text in semi-redundant sequences of max length characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the sequences into an array called $x$ and the list of next characters into an array called $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data/labels loaded in, we can finally construct our LSTM neural network. To do this, use the Keras sequential LSTM module which we have imported in the first cell block. If you're having trouble constructing the model, please refer to the following LSTM example post or ask a demonstrater for some hints. [LSTM example](https://medium.com/@dclengacher/keras-lstm-recurrent-neural-networks-c1f5febde03d) \n",
    "\n",
    "**Tips**\n",
    "-  Use a `Sequential` model.\n",
    "-  Use the keras `LSTM` layers. Try just 1 at first with 128+ cells (basically neurons).\n",
    "-  Number of neurons in the final `Dense` layer should be equivalent to the total number of characters we want to predict.\n",
    "-  Activation function in the final layer should be **softmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile your model using a keras [optimizer](https://keras.io/optimizers/) of your choice. \n",
    "\n",
    "**Tips**\n",
    "-  The `RMSprop` optimizer generally works well here.\n",
    "-  Use the `categorical_crossentropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your model (using `x` and `y` defined above). This will take about 3-5 minutes per epoch on a CPU. Will run much faster on a GPU in Google Colabs.\n",
    "\n",
    "**Tips**\n",
    "-  start with a batch size of 128 and number of epochs of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the following cell below to sample a prediction from the LSTM network after every 1 epoch of training. In order to use this, you can set the `print_callback` variable below as a callback in your `model.fit` function. \n",
    "\n",
    "`model.fit(x,y,batch_size=128,epochs=10,callback=your_callback_variable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "        \n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with hyperparameters such as batch size, number of epochs, optimizer, number of LSTM layers, etc. If you're feeling especially ambitious, you can also try loading in and training over other datasets from the following [Project Gutenberg](https://www.gutenberg.org/ebooks/search/%3Fsort_order%3Ddownloads). Project Gutenberg has a large number of .txt free books which are no longer protected by copyright. An example of a link I might download a book from can be found at the following [page](http://www.gutenberg.org/cache/epub/11/pg11.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Using GANs for Generating Hand-Written Digit Images\n",
    "\n",
    "**GAN (Generative Adversarial Network)** is a framework proposed by Ian Goodfellow, Yoshua Bengio and others in 2014.  \n",
    "\n",
    "A GAN can be trained to generate images from random noises.  For example, we can train a GAN on MNIST (hand-written digits dataset) to generate digit images that look like hand-written digit images from MNIST, which could be used to train other neural networks.\n",
    "\n",
    "The code in this notebook is based on the **GAN MNIST example in TensorFlow by Udacity** which uses TensorFlow, whereas this uses Keras on top of TensorFlow for more straightforward construction of networks.  Many of the ideas on training are from **How to Train a GAN? Tips and tricks to make GANs work**.\n",
    "\n",
    "<img src=\"images/GANs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "MNIST is a well known database of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, Activation, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below will download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine sample images.  We use 'gray' color map since it has no color information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All MNIST digit images come in 28x28 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = X_train[17]\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.title(sample.shape)\n",
    "plt.imshow(sample, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum and maximum sizes of MNIST image data is 0 and 255 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "We want to build a generator that generates realistic hand-written images.  \n",
    "\n",
    "<img src=\"images/generator_ex.png\">\n",
    "\n",
    "The input to the generator is called a 'latent sample' which is basically just a series of randomly generated numbers. These numbers will hopefully end up being mapped through the generator to some image which looks a lot like a handwritten number.  We can use either a normal distribution or a uniform distribution. We'll use a normal distribution for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_latent_samples(n_samples, sample_size):\n",
    "    #return np.random.uniform(-1, 1, size=(n_samples, sample_size))\n",
    "    return np.random.normal(loc=0, scale=1, size=(n_samples, sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the function above to make one sample of a hundred random numbers. We will use this function later to make our GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is a simple fully connected neural network with one hidden layer with the leaky ReLU activation.  It takes one latent sample (100 values) and produces 784 (=28x28) data points which represent a digit image.\n",
    "\n",
    "Go ahead and try making your own generator using the same Keras sequential model as you have used in exercise 1.\n",
    "\n",
    "**Tips**\n",
    "-  Use the `Sequential model` and set it equal to a unique variable name (generator).\n",
    "-  Set your input shape to be 784. This is because we will be flattening our 2D 28x28 image into a 1D 784 long vector.\n",
    "-  Use the `Dense` keras layer along with a `LeakyReLU` activation function.\n",
    "-  In the final `Dense` layer, use a `tanh` activation function.\n",
    "-  Check to see if your network looks alright by using the `model.summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last activation is **tanh**.  It means that we need to rescale the MNIST images to be between -1 and 1. We'll do this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the generator can only produce garbages.\n",
    "\n",
    "<img src=\"images/generator_ex2.png\">\n",
    "\n",
    "As such, the generator needs to learn how to generate realistic hand-written images from the latent sample (randomly generated numbers).\n",
    "\n",
    "How to train this generator?  That is the question tackled by GAN.\n",
    "\n",
    "Before talking about GAN, we'll talk about the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator takes a digit image and classifies whether an image is real (1) or not (0).  \n",
    "\n",
    "If the input image is from the MNIST database, the discriminator should classify it as real.\n",
    "\n",
    "<img src=\"images/discrim_ex1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input image is from the generator, the discriminator should classify it as fake.\n",
    "\n",
    "<img src=\"images/discrim_ex2.png\">\n",
    "\n",
    "The discriminator is a simple fully connected neural network with one hidden layer with the leaky ReLU activation. Go ahead and try setting up the discriminator model on your own below.\n",
    "\n",
    "**Tips**\n",
    "-  Use the `Sequential model` and set it equal to a unique variable name (discriminator).\n",
    "-  Set your input shape to be 784. This is because we will be flattening our 2D 28x28 image into a 1D 784 long vector.\n",
    "-  Use the `Dense` keras layer along with a `LeakyReLU` activation function.\n",
    "-  In the final `Dense` layer, use a `sigmoid` activation function.\n",
    "-  Check to see if your network looks alright by using the `model.summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last activation is **sigmoid** to tell us the probability of whether the input image is real or not.\n",
    "\n",
    "We train the discriminator using both the MNIST images and the images generated by the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "We connect the generator and the discriminator to produce a GAN.  \n",
    "\n",
    "It takes the latent sample, and the generator inside GAN produces a digit image which the discriminator inside GAN classifies as real or fake.\n",
    "\n",
    "<img src=\"images/gan_ex1.png\">\n",
    "\n",
    "If the generated digit image is so realistic, the discriminator in the GAN classifies it as real, which is what we want to achieve.\n",
    "\n",
    "We set the discriminator inside the GAN not-trainable, so it is merely evaluating the quality of the generated image.  The label is always 1 (real) so that if the generator fails to produce a realistic digit image, its cost becomes high, and when the back-propagation occurs in GAN, the weights in the generator network gets updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maintain the same shared weights with the generator and the discriminator.\n",
    "gan = Sequential([\n",
    "    generator,\n",
    "    discriminator\n",
    "])\n",
    "\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the GAN internally uses the same generator and the discriminator models.  The GAN maintains the same shared weights with the generator and the disriminator.  Therefore, training the GAN also trains the generator.  However, we do not want the discriminator to be affected while training the GAN.\n",
    "\n",
    "<img src=\"images/gan_ex2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the discriminator and the GAN in turn and repeat the training many times until both are trained well.  \n",
    "\n",
    "<img src=\"images/gan_ex3.png\">\n",
    "\n",
    "While training the GAN, the back-propagation should update the weights of the generator but not the discriminator.\n",
    "\n",
    "As such, we need a way to make the discriminator trainable and non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(model, trainable):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_trainable(discriminator, False)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_trainable(discriminator, True)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function combines everything we have discussed so far to build the generator, discriminator, and GAN models and also compile them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_simple_GAN(sample_size, \n",
    "                    g_hidden_size, \n",
    "                    d_hidden_size, \n",
    "                    leaky_alpha, \n",
    "                    g_learning_rate,\n",
    "                    d_learning_rate):\n",
    "    K.clear_session()\n",
    "    \n",
    "    generator = Sequential([\n",
    "        Dense(g_hidden_size, input_shape=(sample_size,)),\n",
    "        LeakyReLU(alpha=leaky_alpha),\n",
    "        Dense(784),        \n",
    "        Activation('tanh')\n",
    "    ], name='generator')    \n",
    "\n",
    "    discriminator = Sequential([\n",
    "        Dense(d_hidden_size, input_shape=(784,)),\n",
    "        LeakyReLU(alpha=leaky_alpha),\n",
    "        Dense(1),\n",
    "        Activation('sigmoid')\n",
    "    ], name='discriminator')    \n",
    "    \n",
    "    gan = Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "    \n",
    "    discriminator.compile(optimizer=Adam(lr=d_learning_rate), loss='binary_crossentropy')\n",
    "    gan.compile(optimizer=Adam(lr=g_learning_rate), loss='binary_crossentropy')\n",
    "    \n",
    "    return gan, generator, discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "We need to flatten the digit image data as the fully connected input layer expects that.  Also, as the generator uses the **tanh** activation in the output layer, we scale all the MNIST images to have values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x):    \n",
    "    x = x.reshape(-1, 784) # 784=28*28\n",
    "    x = np.float64(x)\n",
    "    x = (x / 255 - 0.5) * 2\n",
    "    x = np.clip(x, -1, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_real = preprocess(X_train)\n",
    "X_test_real  = preprocess(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprocessing\n",
    "\n",
    "We also need a function to reverse the preprocessing so that we can display generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deprocess(x):\n",
    "    x = (x / 2 + 1) * 255\n",
    "    x = np.clip(x, 0, 255)\n",
    "    x = np.uint8(x)\n",
    "    x = x.reshape(28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "for i in range(20):\n",
    "    img = deprocess(X_train_real[i])\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "The labels are 1 (real) or 0 (fake) in 2D shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labels(size):\n",
    "    return np.ones([size, 1]), np.zeros([size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is 10 sets of real and fake label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_real_10, y_fake_10 = make_labels(10)\n",
    "\n",
    "y_real_10, y_fake_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we prepare the labels for training and evaluation using the train batch size and the test size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing\n",
    "\n",
    "One last point before we start training is the label smoothing which makes the discriminator generalize better.\n",
    "\n",
    "For the real digit images, the labels are all 1s.  However, when we train the discriminator, we use a value slightly smaller than 1 with the real digit images.  Otherwise, the discriminator might overfit to the training data and rejects anything else that is slightly different from the training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We repeat the following to make both the discriminator and the generator better and better:\n",
    "\n",
    "* Prepare a batch of real images\n",
    "* Prepare a batch of fake images generated by the generator using latent samples\n",
    "* Make the discriminator trainable\n",
    "* Train the discriminator to classify the real and fake images\n",
    "* Make the discriminator non-trainable\n",
    "* Train the generator via the GAN\n",
    "\n",
    "When training the generator via the GAN, the expect labels are all 1s (real).  Initially, the generator produces not very realistic images so the discriminator classifies them as 0s (fake), which causes the back-propagation to adjust the weights inside the generator.  The discriminator is not affected as we set it non-trainable in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "sample_size     = 100     # latent sample size (i.e., 100 random numbers)\n",
    "g_hidden_size   = 128\n",
    "d_hidden_size   = 128\n",
    "leaky_alpha     = 0.01\n",
    "g_learning_rate = 0.0001  # learning rate for the generator\n",
    "d_learning_rate = 0.001   # learning rate for the discriminator\n",
    "epochs          = 100\n",
    "batch_size      = 64      # train batch size\n",
    "eval_size       = 16      # evaluate size\n",
    "smooth          = 0.1\n",
    "\n",
    "# labels for the batch size and the test size\n",
    "y_train_real, y_train_fake = make_labels(batch_size)\n",
    "y_eval_real,  y_eval_fake  = make_labels(eval_size)\n",
    "\n",
    "# create a GAN, a generator and a discriminator\n",
    "gan, generator, discriminator = make_simple_GAN(\n",
    "    sample_size, \n",
    "    g_hidden_size, \n",
    "    d_hidden_size, \n",
    "    leaky_alpha, \n",
    "    g_learning_rate,\n",
    "    d_learning_rate)\n",
    "\n",
    "losses = []\n",
    "for e in range(epochs):\n",
    "    for i in range(len(X_train_real)//batch_size):\n",
    "        # real MNIST digit images\n",
    "        X_batch_real = X_train_real[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        # latent samples and the generated digit images\n",
    "        latent_samples = make_latent_samples(batch_size, sample_size)\n",
    "        X_batch_fake = generator.predict_on_batch(latent_samples)\n",
    "        \n",
    "        # train the discriminator to detect real and fake images\n",
    "        make_trainable(discriminator, True)\n",
    "        discriminator.train_on_batch(X_batch_real, y_train_real * (1 - smooth))\n",
    "        discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
    "\n",
    "        # train the generator via GAN\n",
    "        make_trainable(discriminator, False)\n",
    "        gan.train_on_batch(latent_samples, y_train_real)\n",
    "    \n",
    "    # evaluate\n",
    "    X_eval_real = X_test_real[np.random.choice(len(X_test_real), eval_size, replace=False)]\n",
    "    \n",
    "    latent_samples = make_latent_samples(eval_size, sample_size)\n",
    "    X_eval_fake = generator.predict_on_batch(latent_samples)\n",
    "\n",
    "    d_loss  = discriminator.test_on_batch(X_eval_real, y_eval_real)\n",
    "    d_loss += discriminator.test_on_batch(X_eval_fake, y_eval_fake)\n",
    "    g_loss  = gan.test_on_batch(latent_samples, y_eval_real) # we want the fake to be realistic!\n",
    "    \n",
    "    losses.append((d_loss, g_loss))\n",
    "    \n",
    "    print(\"Epoch: {:>3}/{} Discriminator Loss: {:>6.4f} Generator Loss: {:>6.4f}\".format(\n",
    "        e+1, epochs, d_loss, g_loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilizing GAN\n",
    "\n",
    "As it turns out, training a GAN is quite hard, and there are many tricks and heuristics required.  It is because the discriminator and the generator are not cooperating and individually learning to predict better.  \n",
    "\n",
    "For example, the generator might learn to fool the discriminator with garbage.   Ideally, the discriminator should learn earlier than the generator so that it can classify images accurately.\n",
    "\n",
    "Therefore, I used different learning rates for the generator and the discriminator.  I wanted to slow down the generator learning so that the discriminator learns to classify well.\n",
    "\n",
    "I am not 100% certain if this is a generally good strategy to use but it does seem to work in this project.\n",
    "\n",
    "As the generator learns more and the loss decreases, the discriminator's loss increases.  I kind of see equilibrium around 80-90 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Failures\n",
    "\n",
    "* The discriminator loss = 0 means something is wrong\n",
    "* When things are working, the discriminator loss has low variance and goes down over time.\n",
    "* When things are not working, the discriminator loss has huge variance and spiking\n",
    "* If the generator loss steadily decreases, it is fooling the discriminator with garbage.\n",
    "\n",
    "\n",
    "### Don't balance via loss statistics \n",
    "\n",
    "* i.e., don't try to find a (number of G/number of D) schedule to uncollapse training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Generator\n",
    "\n",
    "Now we generates some digit images using the trained generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_samples = make_latent_samples(20, sample_size)\n",
    "generated_digits = generator.predict(latent_samples)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(20):\n",
    "    img = deprocess(generated_digits[i])\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not outstanding as we are using simple networks.  Deep Convolutional GAN (aka DCGAN) would produce better results than this. Now that you've had a brief introduction to GANs, go ahead and try playing around with the overall structure of the network (adding layers, different activation functions, etc.) and tunning various hyper parameters (number of epochs, batch size, learning rate, etc.). Happy GANing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/xkcd_comic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
