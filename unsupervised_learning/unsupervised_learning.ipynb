{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Unsupervised Learning\n",
    "\n",
    "Unsupervised machine learning is when the user steps back and trusts the computer's intuition in finding patterns and correlations within the data. We have discussed different methods for classical unsupervised learning (pre-deep learning) and will utilise these on an artificial dataset to see how they work and where they fall short. To do this we will be using various modules from the `scikit-learn` python package. Like all machine learning, classical unsupervised learning aims to approximate a function\n",
    "\n",
    "$$ y = f (x) $$\n",
    "\n",
    "where $y$ is the output and $x$ is the data. However, in the unsupervised case, the data $x$ is known but the outcomes $y$ are not known. This is determined by what kind of algorithm you want to apply to your data: whether it be clustering, dimensionality reduction or classification. Unsupervised learning puts an emphasis on hyperparameter tuning to understand the results you get and the results you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is to be used for the first set of exercises is saved to this repository in the `data.npz` file which is the same `.npz` file format we used last week. Load in this data and inspect it.\n",
    "\n",
    "* What is the shape of the data?\n",
    "* What does this tell you about the number of features that describes the data?\n",
    "* What do 2D projections of the of the data look like (try plotting the different combinations of axes in 2D)?\n",
    "* How many different types of data is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data.npz\")[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique which finds the *natural* basis for the data and finds the affine transformation required to translate to this basis in which each prinicpal component has maximum variance. Each principal component can be thought of as a linear combination of the features defining the data.\n",
    "\n",
    "The first thing to do is to find out the importance of each of the n principal components for our dataset where an n-dimensional dataset can be described by n principal components. This can be done by applying a PCA fit to our dataset and looking at the `explained_variance_ratio_` attribute of our object.\n",
    "\n",
    "The `scikit-learn` documentation for PCA can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the results of `explained_variance_ratio_` against the label for each principal component using a bar chart. This will show us what principal components are important for describing the data in the dataset. That is, we want to be able to express the data, $D$ as\n",
    "\n",
    "$$ D \\approx \\sum_{i = 1}^{m} d_{i} \\hat{x}_{i}$$\n",
    "\n",
    "where m<n, $d_{i}$ is the data value in the direction of the ith principal component and $\\hat{x}_{i}$ is the unit vector for the ith principal component. An example code for plotting the principal components is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you may want to plot the most important principal components on a linear to see the relative importance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good rule of thumb is that any sum of principal components that corresponds to more than 95% of the variance in the system are good enough to describe your system of data. That is, we discard the dimensions that make up very little of the variation in our data. We can then use the PCA object to reduce the size of the dataset to be described by the samples expressed in the basis of the most important principal components (the number of which will be set by you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully reducing the dimensions of your data, try plotting the permutations of the principal components to see which gives you the best view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to be answered:**\n",
    "\n",
    "1. What is the best permuted visualisation of the data in terms of the principal components that encompass the most variance?\n",
    "2. Do 2D projections of the original data make it obvious where the prinipal components lie?\n",
    "3. What led you to the choices you made for the most important principal components of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: K-Means Clustering\n",
    "\n",
    "Now that we can see what the data looks like when expressed in terms of its most important principal components, we can see the true separation of the classes. Our next job is to cluster the data using two different algorithms: K-Means and hierarchical clustering.\n",
    "\n",
    "K-Means clustering is similar to the K-nearest neighbours classification scheme we saw in the supervised portion of the course. We define K different clusters that we suspect to be in our dataset. These clusters are defined by the mean of the data within them $\\mu_{j}$ which is often called the cluster centroid. These centroids are randomly initialised and the algorithm aims to place them at the minimum of the inertia of the system\n",
    "\n",
    "$$ \\sum_{i = 0}^{n} \\text{min} || x_{i} - \\mu_{j} ||_{\\mu_{j} \\in C}^{2} $$\n",
    "\n",
    "that is for each cluster mean, the algorithm assigns a point to that cluster based on if it has the minimum distance to it and arranges the clusters such that each point is only closest to one centroid. The algorithm does this in three steps:\n",
    "\n",
    "1. Assign each point to a centroid.\n",
    "2. Create new centroids by taking the average of all points in the clusters.\n",
    "3. The difference between these centroids is then calculated and steps 1 & 2 are repeated until this difference is below the defined threshold -- i.e. until the centroids are stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to be answered:**\n",
    "\n",
    "1. What happens to the positions of the centroids when they are initialised in different ways?\n",
    "2. Does increasing/decreasing the number of centroids change the clusters in the way you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Hierarchical Clustering\n",
    "\n",
    "Here we use agglomerative clustering which is where we start with each data point being a cluster, and with our defined distance metric calculate the pairwise distance between each points. Following this, we take the two closest clusters and via our linkage method and merge them to become a new cluster. This algorithm concludes when all data points are part of one large cluster.\n",
    "\n",
    "Note: *divisive* clustering also exits. This is where we start with all data points being in one cluster and do the reverse. However, this has problems when splitting the data into smaller clusters as it is difficult to define the right heuristic to use to separate these clusters. As a result, agglomerative clustering is the preferred method.\n",
    "\n",
    "Try different linkage methods, affinities and cluster numbers and see how changing these hyperparameters changes the clustering results. It should be obvious from the previous plot what the number of clusters should be but this may not always be obvious with your data so it's nice to see how the algorithm works with more or less clusters than is actually needed. Documentation for agglomerative clustering can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dendrograms](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html) are a good way to visualise the connections made by your agglomerative clustering algorithm. Perform clustering several times using different combinations of linkages and metrics to see which gives you the best answer. The best answer can be seen in the dendrogram i.e. make plots of each dendrogram and see which gives you the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of trying different linkage methods with different metrics here is to see what the minimum distance with respect to the total distance needed by the linkage method and metric is which gives the desired number of clusters. For data in which the number of clusters is unknown, the goal of the dendrogram is to see where there is a large increase in distance in forming clusters as this indicates the linkage of clusters that should be distinct.\n",
    "\n",
    "Once you have used the dendrogram to identify the number of clusters in the data, use the [`fcluster`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html) function to define your clusters and plot them with a different colour representing each label. Try this for each of the combinations of linkages and metrics you done previously as this will show more clearly the correct combination to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to be answered:**\n",
    "\n",
    "1. How do you use a dendrogram to determine the number of clusters of in a dataset?\n",
    "2. How can you use a dendrogram to determine the right linkage method/metric to be used for hierarchical clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Clustering with PCA\n",
    "\n",
    "There are many different clustering techniques with K-means and agglomerative being the most simple and easy to use with agglomerative being the most effective (in my opinion). However, there are ways to identify clusters in the data without using a clustering algorithm. This is what the following example is for.\n",
    "\n",
    "After performing PCA on our original data, we can take the two most important principal components and plot the data. From here we cans see the clusters in the data clearly, however what if we only wanted to express our data in terms of one of the two of these principal components. Would we still be able to see the clusters?\n",
    "\n",
    "To do this, we plot [histograms](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) of the data projected onto 1D which shows how the data is spread out over these principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to be answered:**\n",
    "\n",
    "1. How many clusters do you see when projecting the data onto one principal component?\n",
    "2. Is PCA a suitable algorithm for clustering in the case of this data or should we use a different method?\n",
    "3. Can you think of a dataset where this kind of clustering would work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Random Forest Regression\n",
    "\n",
    "This next exercise is to use a random forest approach to make estimations about a dataset. Random forests are collections of decision trees that are applied to the data. In classification, the results of these decision trees is then put to a majority vote between the trees to give the classification. In regression, each decision tree will come up with a prediction for the variable and the forest will take an average of these to get the overall prediction from the random forest.\n",
    "\n",
    "We will generate a dataset described by 2 features and fit a random forest regressor to it to see how well points are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=500,n_features=2,n_informative=2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we are trying to learn a function that is\n",
    "\n",
    "$$ y = f(x_{1}, x_{2}) $$\n",
    "\n",
    "where $X = (x_{1}, x_{2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
